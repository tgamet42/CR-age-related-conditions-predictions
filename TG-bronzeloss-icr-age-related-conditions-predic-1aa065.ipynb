{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6962eaf1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-30T01:13:14.866849Z",
     "iopub.status.busy": "2023-05-30T01:13:14.866474Z",
     "iopub.status.idle": "2023-05-30T01:13:14.87867Z",
     "shell.execute_reply": "2023-05-30T01:13:14.876959Z",
     "shell.execute_reply.started": "2023-05-30T01:13:14.866822Z"
    },
    "papermill": {
     "duration": 0.015061,
     "end_time": "2023-08-11T01:39:25.353448",
     "exception": false,
     "start_time": "2023-08-11T01:39:25.338387",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Lesson Learned, this had a private scored of 0.41 between Sliver to Bronze score ranges, if I had just let it pick the best two scores.\n",
    "\n",
    "# TRY old school RFC and XGBoost on Gamma to see if it outperforms the current MLP on training CSV (which has a .2 ish, once scored .17).\n",
    "\n",
    "# ICR Age Related Conditions Prediction of Class 1 (has one or more conditions, or Class 0 does not have conditions)\n",
    "# Author, Thomas Gamet, released under the Apache 2.0 license\n",
    "\n",
    "Not yet using greeks and currently filling in Null and NaN with standard deviation and mean based normalization.\n",
    "Three models were experimented with:\n",
    "1. Multi Layer Perceptron - seems the most promising at 128 and 64 neuron layers\n",
    "2. Convolutional Neural Network - several attempts made, underperformed MLP\n",
    "3. Long Short Term Memory Neural Network - underperformed MLP\n",
    "\n",
    "There are two versions of the balanced_log_loss implemented for tensor flow.\n",
    "One works with MLP and CNN which has dimensionality 2 for y predictions.\n",
    "The other is for LSMT which uses dimensionality 3 for y predictions.\n",
    "\n",
    "Key notes:\n",
    "1. Tuning with drops of weights causes division by 0 and out of bounds log calculations, just don't bother.\n",
    "2. All batch sizes are set to all training data so that we never run without having both class 1 and class 0. \n",
    "2.1. If a batch is missing all of one or the other you'll get a 0 in N0 or N1 which will cause problems.\n",
    "2.2. I have not experimented with creating my own stratified batches.\n",
    "3. The balanced_log_loss was updated to follow guidance from the Contest page.\n",
    "3.1. It was further updated on 6/24/2023 to work for N>=2 categories.\n",
    "\n",
    "Plan:\n",
    "1. See which model shows best initial promise - so far MLP (two layers, recommending between 90 and 128 nodes - currently running with 128 in the 1st layer, and 64 in the second layer).\n",
    "2. See if means and stddev do better than 0.0 in filling NaN values in features (done and yes).\n",
    "3. Can greeks.csv help do a better job of feature preparation (maybe)?\n",
    "4. SMOTE is non-trivial, using impbalance over_sample module to generate synthetic numeric features, then tuning validation data selection, epochs, and features. SMOTE did not work out, scores became 2x or more times worse.\n",
    "5. Observed that the Greek's data for Gamma maps to Class 0 for Gamma categories M and N, and to class 1 for all other 6 categories. \n",
    "5.1. First pass to see if Gamma can provide a corrective prediction failed.\n",
    "5.2. I am considering alternative approaches to Classifying Gamma (until its classification outperforms the train.csv set it is likely not yet suitable for a voter role).\n",
    "6. Are the features causing any confusion - do we have too many (maybe, we are trying 14 features based on MIscore>0.04 (based on analysis of 9 with MIscore>0.05 and 19 with MIscore>0.03 not being significantly better)).\n",
    "6.1. Possibly over tuning on 6/3/2023 - added highest 38 MI scores less 'AH' which caused log loss to increase (become worse) as features from 33 to 38 were added.\n",
    "6.2. May search the first 33 features for any which when removed improve the internal balanced log loss???\n",
    "7. Do we need stability - make an ensemble work for just MLP (running 5 times, but instead of averaging I'm going to test if the ends or middle are best).\n",
    "8. Tune MLP.\n",
    "8.1. Tracking this in the MLP declaration and new MI scoring code section\n",
    "8.2. Competion is focused on Public score which is with the real validation set\n",
    "8.3. Must use all training data to optimize Public score and graph its success, however for stopping conditions a good split was determined for 0.05 testing data.\n",
    "8.4. Tune the model after finding cross over of training loss against public score\n",
    "8.5. A voter for the 5 selected models (starting at basemodel) was written and the hope is that best probability can be chosen from the 3 to 5 agreeing votes based upon the size of the majority vote (unanimous uses the highest probability, 4 of 5 majority vote uses the second highest probability, and a simple vote uses the middle (also second of 3 of 5) probabilities.) \n",
    "9. Try to tune CNN also - notes left elsewhere, it under performed MLP\n",
    "10. Try to tune LSTM also - the long short term memory does not seem to help - and the scores are less favorable than MLP\n",
    "11. See if an ensemble of the two best models helps. Yes, we are voting on an ensemble of 5.\n",
    "\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3a4e549",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-11T01:39:25.380698Z",
     "iopub.status.busy": "2023-08-11T01:39:25.380220Z",
     "iopub.status.idle": "2023-08-11T01:39:25.889361Z",
     "shell.execute_reply": "2023-08-11T01:39:25.887183Z"
    },
    "papermill": {
     "duration": 0.525955,
     "end_time": "2023-08-11T01:39:25.891998",
     "exception": true,
     "start_time": "2023-08-11T01:39:25.366043",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/input/icr-identify-age-related-conditions/train.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m greeks \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/input/icr-identify-age-related-conditions/greeks.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/input/icr-identify-age-related-conditions/test.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/train.csv')\n",
    "greeks = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/greeks.csv')\n",
    "test = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/test.csv')\n",
    "submission_df = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/sample_submission.csv')\n",
    "print(train.head())\n",
    "print(greeks.head())\n",
    "print(test.head())\n",
    "print(train.describe())\n",
    "print(greeks.describe())\n",
    "print(test.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cd187e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Normalize the training data before reorganizing it for batch processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78700d0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Study of Greeks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23985a47",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "The standardization in the next two code blocks came from Chris Deotte.\n",
    "It is a clean implementation that ensures the standardization used in\n",
    "training is also used on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa5664f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T20:23:59.229629Z",
     "iopub.status.busy": "2023-06-25T20:23:59.229253Z",
     "iopub.status.idle": "2023-06-25T20:23:59.476559Z",
     "shell.execute_reply": "2023-06-25T20:23:59.475443Z",
     "shell.execute_reply.started": "2023-06-25T20:23:59.229598Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train['EJ'] = train.EJ.map({'A':0.0,'B':1.0})\n",
    "train['EJ'].astype(float)\n",
    "\n",
    "FEATURES = []\n",
    "# SAVE MEANS, STDS, NANS FOR TEST INFERENCE LATER\n",
    "means = {}; stds = {}; nans = {}\n",
    "\n",
    "for c in train.columns[1:-1]:\n",
    "    m = train[c].mean()\n",
    "    means[c] = m\n",
    "    s = train[c].std()\n",
    "    stds[c] = s\n",
    "    train[c] = (train[c]-m)/s\n",
    "    n = train[c].min() - 0.5\n",
    "    nans[c] = n\n",
    "    train[c] = train[c].fillna(n)\n",
    "    FEATURES.append(c)\n",
    "    \n",
    "print(train.describe())\n",
    "print(train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1bd7c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-13T01:23:47.322282Z",
     "iopub.status.busy": "2023-06-13T01:23:47.321949Z",
     "iopub.status.idle": "2023-06-13T01:23:49.456815Z",
     "shell.execute_reply": "2023-06-13T01:23:49.455665Z",
     "shell.execute_reply.started": "2023-06-13T01:23:47.322255Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# scores on features used became worse, the synthetic data produced herein\n",
    "# is not conducive / helpful, if anything it appears to be harmful.\n",
    "\n",
    "# WARNING: If we stop dropping column 'Id' here, then we must uncomment where 'Id' is dropped further on\n",
    "# This new code block is attempting to use the text book SMOTE which hopefully helps get closer to a quality score\n",
    "\n",
    "from imblearn.over_sampling import SMOTE # fortunately this is already pip installed or otherwise in the Docker image\n",
    "\n",
    "strain = train.copy()\n",
    "otrain = train.copy()\n",
    "\n",
    "def applySMOTE(df):\n",
    "    #numeric_columns = df.select_dtypes(include=[float,int]).columns\n",
    "    #df_numeric = df[numeric_columns].copy()\n",
    "    #df['Class'] = df['Class'].fillna(0)\n",
    "    y = df['Class']  # Target variable\n",
    "    X = df.drop('Class', axis=1)  # Features (excluding the target variable)\n",
    "    X = df.drop('Id', axis=1)  # Features (excluding the target variable)\n",
    "\n",
    "    # Apply SMOTE to generate synthetic samples\n",
    "    smote = SMOTE(sampling_strategy=0.8, random_state=92833)  # 80% of the minority class size\n",
    "    X_smote, y_smote = smote.fit_resample(X, y)\n",
    "\n",
    "    # Combine the synthetic samples with the original DataFrame\n",
    "    #smotedf = pd.concat([pd.DataFrame(X_smote), pd.DataFrame(y_smote, columns=['Class'])], axis=1)\n",
    "\n",
    "    return X_smote\n",
    "\n",
    "train = applySMOTE(strain)\n",
    "\n",
    "print(train.describe())\n",
    "print(train.head())\n",
    "print(train.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89f62b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T20:23:59.478709Z",
     "iopub.status.busy": "2023-06-25T20:23:59.478243Z",
     "iopub.status.idle": "2023-06-25T20:23:59.654531Z",
     "shell.execute_reply": "2023-06-25T20:23:59.65329Z",
     "shell.execute_reply.started": "2023-06-25T20:23:59.478671Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Greeks Alpha is A for class 0, and B, D, or G for class 1\n",
    "# Beta, Gamma, Delta would need to be predicted from training data and then\n",
    "# their predictions may be used to determine class 0 and class 1 by predicting\n",
    "# Alpha and collapsing Alpha as A to class 0, and Alpha as B, D, G to class 1.\n",
    "# We can also use Alpha alone to diagnose Class 0, and the three types of Class 1.\n",
    "\n",
    "# First step, make sure Greeks data remains inorder with training data\n",
    "# when re-arranged.\n",
    "\n",
    "train['greek_alpha'] = greeks['Alpha']\n",
    "train['greek_beta'] = greeks['Beta']\n",
    "train['greek_gamma'] = greeks['Gamma']\n",
    "train['greek_delta'] = greeks['Delta']\n",
    "\n",
    "print(train.head())\n",
    "print(train.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ec5d39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T20:23:59.657422Z",
     "iopub.status.busy": "2023-06-25T20:23:59.657041Z",
     "iopub.status.idle": "2023-06-25T20:24:05.24169Z",
     "shell.execute_reply": "2023-06-25T20:24:05.240595Z",
     "shell.execute_reply.started": "2023-06-25T20:23:59.657392Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# original reorg to intermix 509 class 0 with 108 class 1 rows\n",
    "strain = train.copy()\n",
    "train = train.drop(train.index) # we want all the columns, but none of the data\n",
    "# now, we are going to refill train, but with a guarantee that every 6th or more\n",
    "# rows has at least one class 1 member\n",
    "strain = strain.sort_values('Class', ascending=True) \n",
    "# this should leave us with 509 rows at for class 0 followed by 108 rows of class 1\n",
    "countdown = 616 # last row is first class 1 to use\n",
    "excess = 5 # 108 - int(509 / 5) - 1\n",
    "for row in range(509):\n",
    "    if row % 5 == 0:\n",
    "        train = train.append(strain.iloc[countdown], ignore_index=True)\n",
    "        countdown -= 1\n",
    "        if row % 10 == 0 and excess > 0: \n",
    "            train = train.append(strain.iloc[countdown], ignore_index=True)\n",
    "            countdown -= 1            \n",
    "            excess -= 1\n",
    "    train = train.append(strain.iloc[row], ignore_index=True)\n",
    "train = train.append(strain.iloc[countdown], ignore_index=True) # the very last class 1\n",
    "\n",
    "print(train.describe())\n",
    "print(train.head())\n",
    "print(train.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daf64e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T20:24:05.243898Z",
     "iopub.status.busy": "2023-06-25T20:24:05.243461Z",
     "iopub.status.idle": "2023-06-25T20:24:05.27562Z",
     "shell.execute_reply": "2023-06-25T20:24:05.274531Z",
     "shell.execute_reply.started": "2023-06-25T20:24:05.24386Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make a copy of the Greek data in the same order as the training data\n",
    "# also pull it out of the dataframe so the existing code will work as designed\n",
    "\n",
    "y_greeks = pd.DataFrame(train,columns=['greek_alpha', 'greek_beta', 'greek_gamma', 'greek_delta'])\n",
    "\n",
    "train.drop('greek_alpha', axis=1, inplace=True)\n",
    "train.drop('greek_beta', axis=1, inplace=True)\n",
    "train.drop('greek_gamma', axis=1, inplace=True)\n",
    "train.drop('greek_delta', axis=1, inplace=True)\n",
    "\n",
    "print(y_greeks.describe())\n",
    "print(y_greeks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5435a20d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T20:24:05.277559Z",
     "iopub.status.busy": "2023-06-25T20:24:05.277228Z",
     "iopub.status.idle": "2023-06-25T20:24:05.469417Z",
     "shell.execute_reply": "2023-06-25T20:24:05.468134Z",
     "shell.execute_reply.started": "2023-06-25T20:24:05.277518Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We perform the exact same feature engineering that we did with train. Specifically we standardize using the same means, stds, and nans.\n",
    "test['EJ'] = test.EJ.map({'A':0,'B':1})\n",
    "for c in test.columns[1:]:\n",
    "    m = means[c]\n",
    "    s = stds[c]\n",
    "    test[c] = (test[c]-m)/s\n",
    "    n = nans[c]\n",
    "    test[c] = test[c].fillna(n)\n",
    "    \n",
    "print(test.describe())\n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2702f3bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T20:24:05.471207Z",
     "iopub.status.busy": "2023-06-25T20:24:05.470892Z",
     "iopub.status.idle": "2023-06-25T20:24:05.480833Z",
     "shell.execute_reply": "2023-06-25T20:24:05.4795Z",
     "shell.execute_reply.started": "2023-06-25T20:24:05.471181Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# select numerical and categorical variables respectively.\n",
    "num_cols = train.select_dtypes(include=['float64']).columns.tolist()\n",
    "cat_cols = train.select_dtypes(include=['object']).columns.tolist()\n",
    "cat_cols.remove('Id') #already removed by SMOTE prep work above\n",
    "print (\"numeric columns\", num_cols)\n",
    "print (\"categorical columns\", cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9440b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T20:24:05.483134Z",
     "iopub.status.busy": "2023-06-25T20:24:05.482834Z",
     "iopub.status.idle": "2023-06-25T20:24:12.357607Z",
     "shell.execute_reply": "2023-06-25T20:24:12.356164Z",
     "shell.execute_reply.started": "2023-06-25T20:24:05.483109Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_df = train #pd.concat([train, test])\n",
    "print(all_df[num_cols].hist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97b7e71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T20:24:12.359535Z",
     "iopub.status.busy": "2023-06-25T20:24:12.359128Z",
     "iopub.status.idle": "2023-06-25T20:24:12.400778Z",
     "shell.execute_reply": "2023-06-25T20:24:12.399635Z",
     "shell.execute_reply.started": "2023-06-25T20:24:12.359497Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    \n",
    "seed_everything(14242) # best try to make runs reproducible - can also tweak this as it affects training splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b0321f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T20:24:12.434217Z",
     "iopub.status.busy": "2023-06-25T20:24:12.433573Z",
     "iopub.status.idle": "2023-06-25T20:24:12.44677Z",
     "shell.execute_reply": "2023-06-25T20:24:12.446033Z",
     "shell.execute_reply.started": "2023-06-25T20:24:12.434174Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# the following works for MLP (Multi Layer Perceptron) and CNN (Convolutional Neural Networks)\n",
    "import tensorflow as tf\n",
    "\n",
    "def balanced_log_loss(y_true, y_pred):\n",
    "    num_classes = 8\n",
    "\n",
    "    # Calculate the number of observations for each class.\n",
    "    N = tf.reduce_sum(y_true, axis=0)\n",
    "\n",
    "    # Calculate the weights for each class (inverse prevalence).\n",
    "    weights = 1.0 / N\n",
    "\n",
    "    # Calculate the predicted probabilities for each class.\n",
    "    p = tf.clip_by_value(y_pred, 1e-15, 1 - 1e-15)\n",
    "\n",
    "    # Calculate the log loss for each class.\n",
    "    log_losses = -tf.reduce_sum(y_true * tf.math.log(p), axis=0)\n",
    "\n",
    "    # Calculate the balanced logarithmic loss.\n",
    "    balanced_log_loss = tf.reduce_mean(weights * log_losses)\n",
    "\n",
    "    return balanced_log_loss\n",
    "\n",
    "#def balanced_log_loss(y_true, y_pred):\n",
    "#    # Calculate the number of observations for each class.\n",
    "#    N_0 = tf.reduce_sum(y_true[:,0])\n",
    "#    N_1 = tf.reduce_sum(y_true[:,1])\n",
    "#\n",
    "#    # Calculate the weights for each class. Thi  is the inverse [of the] prevalence.\n",
    "#    w_0 = 1 / N_0 #N_1 / (N_1 + N_0) #1 / N_0\n",
    "#    w_1 = 1 / N_1 #N_0 / (N_1 + N_0) #1 / N_1\n",
    "#\n",
    "# Calculate the predicted probabilities for each class.\n",
    "#    p_0 = tf.clip_by_value(y_pred[:, 0], 1e-15, 1 - 1e-15)\n",
    "#    p_1 = tf.clip_by_value(y_pred[:, 1], 1e-15, 1 - 1e-15)\n",
    "#\n",
    "#    # Calculate the log loss for each class.\n",
    "#    log_loss_0 = -w_0 * tf.reduce_sum(y_true[:,0] * tf.math.log(p_0))\n",
    "#    log_loss_1 = -w_1 * tf.reduce_sum(y_true[:,1] * tf.math.log(p_1))\n",
    "#\n",
    "#    # Calculate the balanced logarithmic loss.\n",
    "#    balanced_log_loss = (log_loss_0 + log_loss_1) / 2.0\n",
    "#\n",
    "#    return balanced_log_loss\n",
    "\n",
    "# the following works for LSTM (Long Short Term Memory) neural networks\n",
    "def balanced_log_loss_lstm(y_true, y_pred):\n",
    "    # Calculate the number of observations for each class.\n",
    "    N_0 = tf.reduce_sum(y_true[:,0])\n",
    "    N_1 = tf.reduce_sum(y_true[:,1])\n",
    "\n",
    "    # Calculate the weights for each class.\n",
    "    w_0 = 1 / N_0 #N_1 / (N_1 + N_0) #1 / N_0\n",
    "    w_1 = 1 / N_1 #N_0 / (N_1 + N_0) #1 / N_1\n",
    "\n",
    "    # Calculate the predicted probabilities for each class.\n",
    "    p_0 = tf.clip_by_value(y_pred[:, 0, 0], 1e-15, 1 - 1e-15)\n",
    "    p_1 = tf.clip_by_value(y_pred[:, 0, 1], 1e-15, 1 - 1e-15)\n",
    "\n",
    "    # Calculate the log loss for each class.\n",
    "    log_loss_0 = -w_0 * tf.reduce_sum(y_true[:,0] * tf.math.log(p_0))\n",
    "    log_loss_1 = -w_1 * tf.reduce_sum(y_true[:,1] * tf.math.log(p_1))\n",
    "\n",
    "    # Calculate the balanced logarithmic loss.\n",
    "    balanced_log_loss = (log_loss_0 + log_loss_1) / 2\n",
    "\n",
    "    return balanced_log_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce4cc27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T20:24:12.448769Z",
     "iopub.status.busy": "2023-06-25T20:24:12.448117Z",
     "iopub.status.idle": "2023-06-25T20:24:12.474397Z",
     "shell.execute_reply": "2023-06-25T20:24:12.473467Z",
     "shell.execute_reply.started": "2023-06-25T20:24:12.448721Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "#numerical_features = ['AB', 'AF', 'AH', 'AM', 'AR', 'AX', 'AY', 'AZ', #'BC', \n",
    "#                      'BD', 'BN', 'BP', 'BQ', 'BR', 'BZ',\n",
    "#                      'CB', 'CC', 'CD', 'CF', 'CH', #'CL', \n",
    "#                      'CR', 'CS', 'CU', 'CW',\n",
    "#                      'DA', 'DE', 'DF', 'DH', 'DI', 'DL', 'DN', 'DU', 'DV', 'DY',\n",
    "#                      'EB', 'EE', 'EG', 'EH', 'EL', 'EP', 'EU',\n",
    "#                      'FC', 'FD', 'FE', 'FI', 'FL', 'FR', 'FS',\n",
    "#                      'GB', 'GE', 'GF', 'GH', 'GI', 'GL']\n",
    "#categorical_features = ['EJ']\n",
    "#features = numerical_features + categorical_features\n",
    "#str2int_dict = {}\n",
    "#str2int_dict['EJ'] = {'A': 1, 'B': 0}\n",
    "\n",
    "y = pd.DataFrame(all_df['Class'],columns=['class_0'])\n",
    "y['class_1'] = all_df['Class'].fillna(0)\n",
    "y['class_0'] = y['class_1'] * -1 + 1\n",
    "\n",
    "print(y)\n",
    "\n",
    "X_transformed = all_df.copy()\n",
    "X_transformed.drop('Id', axis=1, inplace=True) #already removed by SMOTE above\n",
    "y_mi = X_transformed['Class']\n",
    "X_transformed.drop('Class', axis=1, inplace=True)\n",
    "\n",
    "y = y.astype('float')\n",
    "y_mi = y_mi.copy().astype('int')\n",
    "print(y_mi) \n",
    "\n",
    "print(X_transformed.shape)\n",
    "y = np.array(y) #.reshape(-1,1)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1dc0d7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "##Mutual Information Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20e0ca5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T20:24:12.476247Z",
     "iopub.status.busy": "2023-06-25T20:24:12.475956Z",
     "iopub.status.idle": "2023-06-25T20:24:12.720961Z",
     "shell.execute_reply": "2023-06-25T20:24:12.720006Z",
     "shell.execute_reply.started": "2023-06-25T20:24:12.476221Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import pandas as pd\n",
    "\n",
    "# the following runs were on 1000 epochs with full training set in use\n",
    "# Setting 0.05 as MI score results in  9 features estimated loss .16 accuracy ~75%\n",
    "# Seeting 0.04 as MI score results in 14 features estimated loss .13 accuracy ~85%\n",
    "# Setting 0.03 as MI score results in 19 features estimated loss .08 accuracy ~85%\n",
    "\n",
    "# the following runs were on 1000 epochs with 20% validation\n",
    "# Setting 0.05 as MI score results in  9 features estimated loss .17 accuracy ~75%\n",
    "# Seeting 0.04 as MI score results in 14 features estimated loss .17 accuracy ~75%\n",
    "# Setting 0.03 as MI score results in 19 features estimated loss .18 accuracy ~75%\n",
    "\n",
    "def rank_features_by_mutual_info(tdf: pd.DataFrame, y: pd.Series):\n",
    "    # Calculate the mutual information scores\n",
    "    #mi_scores = tf.keras.metrics.mutual_information(tdf, y)\n",
    "    mi_scores = mutual_info_classif(tdf, y, random_state=4112)\n",
    "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=tdf.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "    return list(mi_scores.items())\n",
    "\n",
    "def top_features(feature_scores, outselector):\n",
    "    top_features_list = []\n",
    "    for feature in feature_scores:\n",
    "        if feature[1] > 0.01 or len(top_features_list) < outselector: # used 0.01 and 34 minimum before SMOTE\n",
    "            top_features_list.append(feature[0])\n",
    "        else:\n",
    "            if len(top_features_list) == 0:\n",
    "                   top_features_list.append(feature[0])\n",
    "            break\n",
    "    return top_features_list\n",
    "\n",
    "totalfeaturesseen = 0\n",
    "miscores = rank_features_by_mutual_info(X_transformed, y_mi)\n",
    "print(miscores)\n",
    "usefeats = top_features(miscores, 28)\n",
    "#usefeats.remove('BR') # this is the 32nd and removing it decreases loss and increases accuracy\n",
    "##usefeats.remove('GE') # this is the 31st and removing it increases loss and decreases accuracy\n",
    "#usefeats.remove('EU') # this is the 30th and removing it decreases loss and increases accuracy\n",
    "#usefeats.remove('AY') # this is the 29th and removing it decreases loss and increases accuracy\n",
    "##usefeats.remove('AX') # this is the 28th and removing it increases loss and decreases accuracy\n",
    "#usefeats.remove('FI') # this is the 27th and removing it decreases loss and decreases accuracy\n",
    "##usefeats.remove('CD ')# this is the 26th and removing it aboutsame loss and aboutsame accuracy\n",
    "##usefeats.remove('AR') # this is the 25th and removing it increased loss and increases accuracy\n",
    "#usefeats.remove('CF') # this is the 24th and removing it decreases loss and increases accuracy\n",
    "#usefeats.remove('BP') # this is the 23th and removing it decreases loss and increases accuracy\n",
    "#usefeats.remove('AH') # MIscore rank 37 (only get's removed if 37 or more are selected)\n",
    "print(\"Total features =\", len(usefeats))\n",
    "print(\"Feaures to use =\", usefeats)\n",
    "\n",
    "X_transformed = X_transformed[usefeats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd5c324",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T20:24:12.722888Z",
     "iopub.status.busy": "2023-06-25T20:24:12.722499Z",
     "iopub.status.idle": "2023-06-25T20:24:13.366934Z",
     "shell.execute_reply": "2023-06-25T20:24:13.365829Z",
     "shell.execute_reply.started": "2023-06-25T20:24:12.72286Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import BatchNormalization, Dense, Dropout, Conv1D, Flatten, Input, MaxPooling1D, concatenate, BatchNormalization\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv1D, Flatten, Input, MaxPooling1D, concatenate, BatchNormalization\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# MLP model original input(56)-dense(128)-dense(128)-Flatten-Dense(2) relu->softmax\n",
    "# Date     , loss, eloss, patience, min_del, epochs/uses ep, val_acc\n",
    "# 5/27/2023, 0.48,  0.2209,     0010, 0.00500, 000225/225    , ~60%\n",
    "# 5/28/2023, 0.41,  0.1576,     0010, 0.00500, 000500/300-350, ~80%\n",
    "# 5/29/2023, 0.32,  0.0850,     0010, 0.00050, 000500/500    , ~85%\n",
    "# 5/30/2023, 0.29,  0.0335,     0010, 0.00010, 000750/750    , ~95%\n",
    "# Reducing to MIscore>0.04 providing 14 features\n",
    "# 5/31/2023, 0.38,  0.1251,     0010, 0.00010, 001000/1000   , ~82%\n",
    "# 6/01/2023, 0.23,  0.1683,     0010, 0.00010, 001000/390    , ~93% MIs>0.01, 0.05test MLP128x128\n",
    "# 6/02/2023, 0.22,  0.1386,     0100, 0.00001, 002500/580,   , ~93% MIs>0.01, 0.05test MLP128x64\n",
    "# 6/03/2023, 0.18,  0.1205,     0100, 0.00001, 002500/400-800, ~97% MIs>0.01, 0.05test MLP128x64 with 5 model voter base 5 and 33 features\n",
    "# 6/04/2023, 0.21,  0.0949,     0100, 0.00001, 002500/400-800, ~96% MIs>0.01, 0.05test MLP128x64 with 5 model voter base 5 and 37of38 features\n",
    "# 6/05/2023, 0.18,  0.1140,     0100, 0.00001, 002500/400-800, ~96% MIs>0.01,5 and 33 fea 0.05test MLP128-160x64 with 5 model voter base features\n",
    "# making it so about every 6th training row is class 1 allows batching 101\n",
    "# 6/06/2023, 0.17,  0.1061,     0020, 0.00100, 002500/175-270, ~96% MIs>0.01, 0.05test MLP128-160x64 with 5 model voter base 5 and 34 features\n",
    "# 6/07/2023, 0.19,  0.0865,     0020, 0.00100, 002500/175-270, ~97% MIs>0.01, 0.05test MLP128-160x64 with 5 model voter base 5 and 34 features less 'BR' batchsize=101\n",
    "# 6/08/2023, 0.19,  0.0231,     0020, 0.00100, 002500/175-270, 100% MIs>0.01, 0.05test MLP128-160x64 with 5 model voter base 5 and 34 features less 'BR', 'EU', 'AY', 'FI, 'CF', and 'BP'\n",
    "# 6/09/2023, 0.19,  0.0788,     0020, 0.00100, 002500/175-270, 100% MIs>0.01, 0.05test MLP128-160x64 with 5 model voter base 5 and 34 features less 'BR', 'EU', 'AY', 'FI, 'CF', and 'BP' stop near val_loss and loss crossing\n",
    "# Either we need a better model or feature engineering - now trying SMOTE with 509 of each class\n",
    "# 6/10/2023, 0.29,  0.0837,     0020, 0.00100, 000200, SMOTE ~98% MIs>0.01, 0.2test-unique MLP128-160x64 with 5 model voter base 5 batchsize 254\n",
    "# 6/11/2023, 0.38, with SMOTE on 43 features with Dense len(usefeats)*1.5...\n",
    "# 6/12/2023, 0.39, with SMOTE on 43 features with Dense 128x64\n",
    "# 6/13/2023, 0.85, with SMOTE on 43 features with Dense len(usefeats) \n",
    "# The SMOTE approach appears to be failing. It is time to see what predictions\n",
    "# the greeks.csv will produce (any one positive means class 1)\n",
    "def MLPmodelBuilder(X_transformed):\n",
    "    inputs = Input(shape=[X_transformed.shape[1]])\n",
    "    dense = Dense(128, activation='relu')(inputs)\n",
    "    #dense = Dropout(.20)(dense)\n",
    "    dense = Dense(64, activation='relu')(dense)\n",
    "    dense = Flatten()(dense)\n",
    "    outputs = Dense(2, activation='softmax')(dense)\n",
    "    return inputs, outputs\n",
    "\n",
    "def MLPmodelBuilderOut3(X_transformed):\n",
    "    inputs = Input(shape=[X_transformed.shape[1]])\n",
    "    dense = Dense(128, activation='relu')(inputs)\n",
    "    #dense = Dropout(.20)(dense)\n",
    "    dense = Dense(64, activation='relu')(dense)\n",
    "    dense = Flatten()(dense)\n",
    "    outputs = Dense(3, activation='softmax')(dense)\n",
    "    return inputs, outputs\n",
    "\n",
    "def MLPmodelBuilderOut8(X_transformed):\n",
    "    inputs = Input(shape=[X_transformed.shape[1]])\n",
    "    dense = Dense(128, activation='relu')(inputs)\n",
    "    #dense = Dropout(.20)(dense)\n",
    "    dense = Dense(64, activation='relu')(dense)\n",
    "    dense = Flatten()(dense)\n",
    "    outputs = Dense(8, activation='softmax')(dense)\n",
    "    return inputs, outputs\n",
    "\n",
    "# Ran experimments with size 32 - 128 increments of 4\n",
    "# Both val of .2 and .5 show that the larger nets do better, but 114 or 86 might do just as well\n",
    "# Val scores suggest targets should be reached in the low 90% accuracy.\n",
    "# With .5 val data, loss functions are 0.15 through val loss 0.33, which is not immpressive, but all training data should do a little better.\n",
    "# Ran val set of .5 without Flattening, and it process a similar result to the Flatten calling version.\n",
    "# Sigmoid DOES NOT PRODUCE probabilities we can use.\n",
    "def MLPmodelBuilderSize(X_transformed, size):\n",
    "    inputs = Input(shape=[X_transformed.shape[1]])\n",
    "    dense = Dense(size, activation='relu')(inputs)\n",
    "    #dense = Dropout(.20)(dense)\n",
    "    dense = Dense(size/2, activation='relu')(dense)\n",
    "    #dense = Dropout(.20)(dense)\n",
    "    #dense = Dense(size/4, activation='relu')(dense) # seems to confuse results - overfit\n",
    "    #dense = Flatten()(dense)\n",
    "    outputs = Dense(2, activation='softmax')(dense) #tried sigmoid\n",
    "    return inputs, outputs\n",
    "\n",
    "# MLP model must try\n",
    "# the highest score yielded a 0.32 (internal around .10) and lowest a 0.33 (internal 0.08)\n",
    "#def MLPmodelBuilder(X_transformed):\n",
    "#    inputs = Input(shape=[X_transformed.shape[1]])\n",
    "#    dense = Dense(42, activation='relu')(inputs)\n",
    "#    #dense = Dense(21, activation='relu')(dense)\n",
    "#    #dense = Dense(114, activation='relu')(dense)\n",
    "#    dense = Flatten()(dense)\n",
    "#    outputs = Dense(2, activation='softmax')(dense)\n",
    "#    return inputs, outputs\n",
    "\n",
    "def CNNmodelBuilderSize(X_transformed, size):\n",
    "    inputs = Input(shape=[X_transformed.shape[1], 1])\n",
    "    conv = Conv1D(size, kernel_size=6, activation='relu')(inputs)\n",
    "    conv = MaxPooling1D(pool_size=2)(conv)\n",
    "    conv = Conv1D(size/2, kernel_size=6, activation='relu')(conv)\n",
    "    conv = MaxPooling1D(pool_size=2)(conv)\n",
    "    #conv = Conv1D(size/4, kernel_size=6, activation='relu')(conv)\n",
    "    #conv = MaxPooling1D(pool_size=2)(conv)\n",
    "    conv = Flatten()(conv)\n",
    "    outputs = Dense(2, activation='softmax')(conv)  \n",
    "    return inputs, outputs\n",
    "\n",
    "def LSTMmodelBuilder(X_transformed):\n",
    "    inputs = Input(shape=(1, X_transformed.shape[1]))\n",
    "    # Let's try an LSTM\n",
    "    lstm = tf.keras.layers.LSTM(len(X_transformed), \n",
    "                                recurrent_activation='relu',\n",
    "                                activation='relu',\n",
    "                                #dropout=.5,\n",
    "                                #recurrent_dropout=.5,\n",
    "                                return_sequences=True)(inputs)\n",
    "    # define the output layer\n",
    "    outputs = Dense(2, activation='softmax')(lstm)\n",
    "    return inputs, outputs\n",
    "\n",
    "# Custom build of neural networks\n",
    "\n",
    "# due to the voter the minimum number of models is now 10, and 15 should be used.\n",
    "# tupple with 0 = MLP, 1 = CNN, 2 = LSTM\n",
    "# due to the voter, all models must be of the same type\n",
    "models = []\n",
    "for i in range(15):\n",
    "    inputs, outputs = MLPmodelBuilder(X_transformed)\n",
    "    #inputs, outputs = MLPmodelBuilderSize(X_transformed, 96+i*4)\n",
    "    #inputs, outputs = CNNmodelBuilderSize(X_transformed, 32+i*2)\n",
    "    #inputs, outputs = LSTMmodelBuilder(X_transformed)\n",
    "    modelx = Model(inputs=inputs, outputs=outputs)\n",
    "    models.append([modelx, 0]) # must change 0 to 2 for LSTM, and optional to 1 for CNN\n",
    "    #inputs, outputs = CNNmodelBuilderSize(X_transformed, 128)\n",
    "    #modelx = Model(inputs=inputs, outputs=outputs)\n",
    "    #models.append([modelx, 1]) # must change 0 to 2 for LSTM, and optional to 1 for CNN\n",
    "\n",
    "print(\"Models are built and ready to fit...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608e421c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T20:24:13.369241Z",
     "iopub.status.busy": "2023-06-25T20:24:13.368683Z",
     "iopub.status.idle": "2023-06-25T20:26:32.876675Z",
     "shell.execute_reply": "2023-06-25T20:26:32.875695Z",
     "shell.execute_reply.started": "2023-06-25T20:24:13.369201Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# This code block compiles and build 15 models for \n",
    "# the main training set of class 0 and class 1\n",
    "# The best answers are between 0.17 and 0.20 log loss\n",
    "\n",
    "#maxepochs = [2500, 2500, 2500, 2500, 2500,  # probably overfits\n",
    "#             2500, 2500, 2500, 2500, 2500,  # 0.1% test data 95% and 0.17ish\n",
    "#             2500, 2500, 2500, 2500, 2500]  # 0.05% test at 98% and 0.05-0.09 batch size 202\n",
    "#maxepochs = [230, 270, 140, 160,175,  # try this with 0.05% test data\n",
    "#             265, 230, 275, 145,310,  # after the full run - 97% and 0.07-0.11\n",
    "#             280, 220, 300, 145, 220]\n",
    "#maxepochs = [100, 90, 80, 110, 120,   # try this with 0.1% test data\n",
    "#             125, 120, 140, 75, 100,  # after full run 93% and .22-0.24\n",
    "#             140, 125, 100, 75, 125]\n",
    "maxepochs = [2000, 2000, 2000, 2000, 2000,  # probably overfits\n",
    "             2000, 2000, 2000, 2000, 2000,  # 0.2% test data with SMOTE procedure for 509 in each class\n",
    "             2000, 2000, 2000, 2000, 2000, 2000]  # \n",
    "devrun = True\n",
    "\n",
    "# This was too hard to tune, especially without validation\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    patience=30, #18, # restore 18, # be very patient, long enough for the cross over\n",
    "    min_delta=0.005,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "# This was too hard to tune, especially without validation\n",
    "early_stopping_lstm = keras.callbacks.EarlyStopping(\n",
    "    patience=10,     # hyper tune these to get loss to stop near 0.12\n",
    "    min_delta=0.0001,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "#train , val = np.split(data.sample(frac = 1) , [int(len(data) * 0.8)])\n",
    "#print(y)\n",
    "#**** next experiment should be to find optimal split or see of smaller batches can work\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_transformed, y, test_size=0.05, random_state=42, stratify=y)\n",
    "\n",
    "## Radical new approach, try tuning with all the data against just the \n",
    "## test with one run per day, and model improvements using early stopping\n",
    "##\n",
    "print (\"Compiling models\")\n",
    "\n",
    "## notes when run with seed 14242\n",
    "## MLP single model 1/1 [==============================] - 0s 40ms/step - loss: 21.1738 - accuracy: 0.9635 - binary_accuracy: 0.9635 - val_loss: 15.7028 - val_accuracy: 0.8952 - val_binary_accuracy: 0.8952\n",
    "## CNN single model 1/1 [==============================] - 0s 83ms/step - loss: 24.7791 - accuracy: 0.9736 - binary_accuracy: 0.9736 - val_loss: 15.8823 - val_accuracy: 0.8468 - val_binary_accuracy: 0.8468\n",
    "## LSTM single model 1/1 [==============================] - 0s 82ms/step - loss: 35.5633 - accuracy: 0.5000 - binary_accuracy: 0.6908 - val_loss: 16.8421 - val_accuracy: 0.5000 - val_binary_accuracy: 0.6613\n",
    "\n",
    "modelno = 0\n",
    "for modeltup in models:\n",
    "    #X_train, X_val, y_train, y_val = train_test_split(X_transformed, y, test_size=0.2, random_state=42+(modelno%2)*100, stratify=y)\n",
    "    model = modeltup[0]\n",
    "    print (\"Compiling and fitting model\", modelno+1)\n",
    "    tf.random.set_seed(91+modelno*79)\n",
    "    #(optimizer='adam', loss = balanced_log_loss)\n",
    "    if modeltup[1] == 2: # this is for LSTM\n",
    "        model.compile(loss=balanced_log_loss_lstm,\n",
    "                optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "                metrics=['accuracy', 'binary_accuracy'])\n",
    "        Xd = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1])) # only for LSTM\n",
    "        Xd_val = np.reshape(X_val, (X_val.shape[0], 1, X_val.shape[1])) # only for LSTM\n",
    "        history = model.fit(\n",
    "            X_transformed, #Xd, \n",
    "            y, #y_train,\n",
    "            epochs = 1500,\n",
    "            batch_size = len(Xd),\n",
    "            validation_data = (Xd_val, y_val),\n",
    "            callbacks=[early_stopping_lstm]\n",
    "        )\n",
    "    else: # this covers both CNN and MLP\n",
    "        model.compile(loss=balanced_log_loss,\n",
    "                optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "                metrics=['accuracy', 'binary_accuracy'])\n",
    "        history = model.fit(\n",
    "            X_train, #X_transformed, #X_train, # for MLP and CNN, else Xd\n",
    "            y_train, #y, #y_train,\n",
    "            epochs = maxepochs[modelno], #2500, #225, \n",
    "            verbose = False, #devrun,\n",
    "            batch_size = 101, # len(X_transformed),\n",
    "            validation_data = (X_val, y_val),\n",
    "            callbacks=[early_stopping]\n",
    "        )\n",
    "    modelno += 1\n",
    "    if devrun:\n",
    "        pd.DataFrame(history.history).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f9917b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T20:26:32.878435Z",
     "iopub.status.busy": "2023-06-25T20:26:32.878131Z",
     "iopub.status.idle": "2023-06-25T20:30:06.392473Z",
     "shell.execute_reply": "2023-06-25T20:30:06.391384Z",
     "shell.execute_reply.started": "2023-06-25T20:26:32.878409Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Alternate Code Block 1 of 4\n",
    "#\n",
    "# This code block compiles and build 15 models for \n",
    "# the training the Greeks output for Beta \n",
    "# We also have blocks for Gamma and Delta\n",
    "# Markdown this block to go back to the original or\n",
    "# keep this 1 of 4 code blocks which predict Greeks and\n",
    "# then use that prediction as features to predict class 0 and \n",
    "# class 1\n",
    "\n",
    "maxepochs = [400, 400, 400, 400, 400,  # probably overfits\n",
    "             400, 400, 400, 400, 400,  # 0.2% test data with SMOTE procedure for 509 in each class\n",
    "             400, 400, 400, 400, 400, 400]  # \n",
    "devrun = True\n",
    "\n",
    "# This was too hard to tune, especially without validation\n",
    "early_stopping_g = keras.callbacks.EarlyStopping(\n",
    "    patience=30, #18, # restore 18, # be very patient, long enough for the cross over\n",
    "    min_delta=0.0005,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "# This was too hard to tune, especially without validation\n",
    "#early_stopping_lstm = keras.callbacks.EarlyStopping(\n",
    "#    patience=10,     # hyper tune these to get loss to stop near 0.12\n",
    "#    min_delta=0.0001,\n",
    "#    restore_best_weights=True,\n",
    "#)\n",
    "\n",
    "gbmodels = []\n",
    "for i in range(15):\n",
    "    inputs, outputs = MLPmodelBuilderOut3(X_transformed)\n",
    "    modelx = Model(inputs=inputs, outputs=outputs)\n",
    "    gbmodels.append([modelx, 0]) # must change 0 to 2 for LSTM, and optional to 1 for CNN\n",
    "\n",
    "print(\"Models are built and ready to fit...\")\n",
    "\n",
    "# rebuilt X_transformed for Greek Beta\n",
    "yclone = y.copy()\n",
    "y_beta = pd.DataFrame(yclone,columns=['A', 'B'])\n",
    "y_beta['C'] = y_beta['B']\n",
    "for row in range(len(y)):\n",
    "    if y_greeks.loc[row,'greek_beta'] == 'A':\n",
    "        y_beta.loc[row,'A'] = 1.0\n",
    "        y_beta.loc[row,'B'] = 0.0\n",
    "        y_beta.loc[row,'C'] = 0.0\n",
    "    elif y_greeks.loc[row,'greek_beta'] == 'B':\n",
    "        y_beta.loc[row,'A'] = 0.0\n",
    "        y_beta.loc[row,'B'] = 1.0\n",
    "        y_beta.loc[row,'C'] = 0.0\n",
    "    else:\n",
    "        y_beta.loc[row,'A'] = 0.0\n",
    "        y_beta.loc[row,'B'] = 0.0\n",
    "        y_beta.loc[row,'C'] = 1.0\n",
    "print(y_beta.head())\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_transformed, y_beta, test_size=0.05, random_state=42, stratify=y)\n",
    "\n",
    "## Radical new approach, try tuning with all the data against just the \n",
    "## test with one run per day, and model improvements using early stopping\n",
    "##\n",
    "print (\"Compiling models\")\n",
    "\n",
    "## notes when run with seed 14242\n",
    "## MLP single model 1/1 [==============================] - 0s 40ms/step - loss: 21.1738 - accuracy: 0.9635 - binary_accuracy: 0.9635 - val_loss: 15.7028 - val_accuracy: 0.8952 - val_binary_accuracy: 0.8952\n",
    "## CNN single model 1/1 [==============================] - 0s 83ms/step - loss: 24.7791 - accuracy: 0.9736 - binary_accuracy: 0.9736 - val_loss: 15.8823 - val_accuracy: 0.8468 - val_binary_accuracy: 0.8468\n",
    "## LSTM single model 1/1 [==============================] - 0s 82ms/step - loss: 35.5633 - accuracy: 0.5000 - binary_accuracy: 0.6908 - val_loss: 16.8421 - val_accuracy: 0.5000 - val_binary_accuracy: 0.6613\n",
    "\n",
    "modelno = 0\n",
    "for modeltup in gbmodels:\n",
    "    #X_train, X_val, y_train, y_val = train_test_split(X_transformed, y, test_size=0.2, random_state=42+(modelno%2)*100, stratify=y)\n",
    "    model = modeltup[0]\n",
    "    print (\"Compiling and fitting model\", modelno+1)\n",
    "    tf.random.set_seed(91+modelno*79)\n",
    "    #(optimizer='adam', loss = balanced_log_loss)\n",
    "    if modeltup[1] == 2: # this is for LSTM\n",
    "        model.compile(loss=balanced_log_loss_lstm,\n",
    "                optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "                metrics=['accuracy', 'binary_accuracy'])\n",
    "        Xd = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1])) # only for LSTM\n",
    "        Xd_val = np.reshape(X_val, (X_val.shape[0], 1, X_val.shape[1])) # only for LSTM\n",
    "        history = model.fit(\n",
    "            X_transformed, #Xd, \n",
    "            y, #y_train,\n",
    "            epochs = 1500,\n",
    "            batch_size = len(Xd),\n",
    "            validation_data = (Xd_val, y_val),\n",
    "            callbacks=[early_stopping_lstm]\n",
    "        )\n",
    "    else: # this covers both CNN and MLP\n",
    "        model.compile(loss=balanced_log_loss,\n",
    "                optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "                metrics=['accuracy'])\n",
    "        history = model.fit(\n",
    "            X_train, #X_transformed, #X_train, # for MLP and CNN, else Xd\n",
    "            y_train, #y, #y_train,\n",
    "            epochs = maxepochs[modelno], #2500, #225, \n",
    "            verbose = False, #devrun,\n",
    "            batch_size = 617, # len(X_transformed),\n",
    "            validation_data = (X_val, y_val),\n",
    "            #callbacks=[early_stopping_g]\n",
    "        )\n",
    "    modelno += 1\n",
    "    if devrun:\n",
    "        pd.DataFrame(history.history).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5a1c65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T20:30:06.394167Z",
     "iopub.status.busy": "2023-06-25T20:30:06.39385Z",
     "iopub.status.idle": "2023-06-25T20:31:42.040296Z",
     "shell.execute_reply": "2023-06-25T20:31:42.039028Z",
     "shell.execute_reply.started": "2023-06-25T20:30:06.394142Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Alternate Code Block 2 of 4 coded for LGBM\n",
    "#\n",
    "# This code block compiles and build 15 models for \n",
    "# the training the Greeks output for Beta \n",
    "# We also have blocks for Gamma and Delta\n",
    "# Markdown this block to go back to the original or\n",
    "# keep this 1 of 4 code blocks which predict Greeks and\n",
    "# then use that prediction as features to predict class 0 and \n",
    "# class 1\n",
    "\n",
    "from sklearn.metrics import log_loss, make_scorer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, StratifiedGroupKFold\n",
    "from lightgbm import LGBMClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def lgb_balanced_log_loss(y_true, y_pred):\n",
    "    N_0 = np.sum(1 - y_true)\n",
    "    N_1 = np.sum(y_true)\n",
    "    p_1 = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "    p_0 = 1 - p_1\n",
    "    log_loss_0 = -np.sum((1 - y_true) * np.log(p_0))\n",
    "    log_loss_1 = -np.sum(y_true * np.log(p_1))\n",
    "    w_0 = 1 / N_0\n",
    "    w_1 = 1 / N_1\n",
    "    balanced_log_loss = 2*(w_0 * log_loss_0 + w_1 * log_loss_1) / (w_0 + w_1)\n",
    "    return balanced_log_loss/(N_0+N_1)\n",
    "\n",
    "def lgb_metric(y_true, y_pred):\n",
    "    return 'balanced_log_loss', lgb_balanced_log_loss(y_true, y_pred), False\n",
    "\n",
    "Xg_transformed = all_df.copy()\n",
    "\n",
    "log_losses = []\n",
    "balanced_log_losses = []\n",
    "\n",
    "feature_importance_df_total = pd.DataFrame()\n",
    "\n",
    "lgbmodels = {}  # key: bag, val: [model] * fold_num\n",
    "\n",
    "lgbm_params = {\"boosting_type\": 'goss'\n",
    "               , \"learning_rate\": 0.06733232950390658\n",
    "               , \"n_estimators\": 50000\n",
    "               , \"early_stopping_round\": 300\n",
    "               , \"random_state\": 42\n",
    "               , \"subsample\": 0.6970532011679706\n",
    "               , \"colsample_bytree\": 0.6055755840633003\n",
    "               , \"class_weight\": 'balanced'\n",
    "               , \"metric\": 'none'\n",
    "               , \"is_unbalance\": True\n",
    "               , \"max_depth\": 8}\n",
    "\n",
    "bag_num = 20\n",
    "n_fold = 5\n",
    "\n",
    "for bag in range(bag_num):\n",
    "    \n",
    "    print(f'########################## bag: {bag} ##########################')\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=n_fold, random_state=42 * bag, shuffle=True)\n",
    "    \n",
    "    lgbmodels[bag] = []\n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(Xg_transformed, greeks['Gamma'])):\n",
    "        # df.loc[test_idx, 'fold'] = fold\n",
    "\n",
    "        train_df = Xg_transformed.iloc[train_idx]\n",
    "        valid_df = Xg_transformed.iloc[test_idx]\n",
    "        valid_ids = valid_df.Id.values.tolist()\n",
    "\n",
    "        X_train, y_train = train_df[usefeats], train_df['Class']\n",
    "        X_valid, y_valid = valid_df[usefeats], valid_df['Class']\n",
    "\n",
    "        lgb = LGBMClassifier(**lgbm_params)\n",
    "\n",
    "        lgb.fit(X_train, y_train, eval_set=(X_valid, y_valid), verbose=False,\n",
    "                eval_metric=lgb_metric)\n",
    "        \n",
    "        # \n",
    "        feature_importances = lgb.feature_importances_\n",
    "        # \n",
    "        feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Split': feature_importances, 'Gain': lgb.booster_.feature_importance(importance_type='gain')})\n",
    "        feature_importance_df['bag'] = bag\n",
    "        feature_importance_df['fold'] = fold\n",
    "        \n",
    "        feature_importance_df_total = pd.concat([feature_importance_df_total, feature_importance_df], axis=0)\n",
    "        \n",
    "        y_pred = lgb.predict_proba(X_valid)\n",
    "\n",
    "        logloss = log_loss(y_valid, y_pred)\n",
    "        balanced_logloss = lgb_balanced_log_loss(y_valid, y_pred[:, 1])\n",
    "        log_losses.append(logloss)\n",
    "        balanced_log_losses.append(balanced_logloss)\n",
    "        \n",
    "        lgbmodels[bag].append(lgb)\n",
    "\n",
    "        print(f\"Bags: {bag}, Fold: {fold}, log loss: {round(logloss, 3)}, balanced los loss: {round(balanced_logloss, 3)}\")\n",
    "\n",
    "print()\n",
    "print(\"Log Loss\")\n",
    "# print(log_losses)\n",
    "print(np.mean(log_losses), np.std(log_losses))\n",
    "print()\n",
    "print(\"Balanced Log Loss\")\n",
    "# print(balanced_log_losses)\n",
    "print(np.mean(balanced_log_losses), np.std(balanced_log_losses))\n",
    "print()\n",
    "\n",
    "plt.hist(log_losses, bins=100) \n",
    "plt.axvline(x=np.mean(log_losses), color='red', linestyle='dashed', label='Mean: {:.2f}'.format(np.mean(log_losses))) \n",
    "plt.axvline(x=np.median(log_losses), color='blue', linestyle='dashed', label='Median: {:.2f}'.format(np.median(log_losses))) \n",
    "plt.legend() \n",
    "plt.title('Histogram of log_losses std: {:.2f}'.format(np.std(log_losses)),size=20) \n",
    "plt.show()\n",
    "\n",
    "plt.hist(balanced_log_losses, bins=100) \n",
    "plt.axvline(x=np.mean(balanced_log_losses), color='red', linestyle='dashed', label='Mean: {:.2f}'.format(np.mean(balanced_log_losses))) \n",
    "plt.axvline(x=np.median(balanced_log_losses), color='blue', linestyle='dashed', label='Median: {:.2f}'.format(np.median(balanced_log_losses))) \n",
    "plt.legend() \n",
    "plt.title('Histogram of balanced_log_losses std: {:.2f}'.format(np.std(balanced_log_losses)),size=20) \n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37abf0c0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Alternate Code Block 2 of 4\n",
    "#\n",
    "# This code block compiles and build 15 models for \n",
    "# the training the Greeks output for Beta \n",
    "# We also have blocks for Gamma and Delta\n",
    "# Markdown this block to go back to the original or\n",
    "# keep this 1 of 4 code blocks which predict Greeks and\n",
    "# then use that prediction as features to predict class 0 and \n",
    "# class 1\n",
    "\n",
    "maxepochs = [1000, 1000, 1000, 1000, 1000,  # probably overfits\n",
    "             1000, 1000, 1000, 1000, 1000,  # 0.2% test data with SMOTE procedure for 509 in each class\n",
    "             1000, 1000, 1000, 1000, 1000, 1000]  # \n",
    "devrun = True\n",
    "\n",
    "# This was too hard to tune, especially without validation\n",
    "early_stopping_g = keras.callbacks.EarlyStopping(\n",
    "    patience=30, #18, # restore 18, # be very patient, long enough for the cross over\n",
    "    min_delta=0.0005,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "# This was too hard to tune, especially without validation\n",
    "#early_stopping_lstm = keras.callbacks.EarlyStopping(\n",
    "#    patience=10,     # hyper tune these to get loss to stop near 0.12\n",
    "#    min_delta=0.0001,\n",
    "#    restore_best_weights=True,\n",
    "#)\n",
    "\n",
    "ggmodels = []\n",
    "for i in range(15):\n",
    "    inputs, outputs = MLPmodelBuilderOut8(X_transformed)\n",
    "    modelx = Model(inputs=inputs, outputs=outputs)\n",
    "    ggmodels.append([modelx, 0]) # must change 0 to 2 for LSTM, and optional to 1 for CNN\n",
    "\n",
    "print(\"Models are built and ready to fit...\")\n",
    "\n",
    "# rebuilt X_transformed for Greek Beta\n",
    "yclone = y.copy()\n",
    "y_gamma = pd.DataFrame(yclone,columns=['G', 'H'])\n",
    "y_gamma['E'] = y_gamma['H']\n",
    "y_gamma['F'] = y_gamma['H']\n",
    "y_gamma['A'] = y_gamma['H']\n",
    "y_gamma['B'] = y_gamma['H']\n",
    "y_gamma['M'] = y_gamma['H']\n",
    "y_gamma['N'] = y_gamma['H']\n",
    "for row in range(len(y)):\n",
    "    if y_greeks.loc[row,'greek_gamma'] == 'G':\n",
    "        y_gamma.loc[row,'G'] = 1.0\n",
    "        y_gamma.loc[row,'H'] = 0.0\n",
    "        y_gamma.loc[row,'E'] = 0.0\n",
    "        y_gamma.loc[row,'F'] = 0.0\n",
    "        y_gamma.loc[row,'A'] = 0.0\n",
    "        y_gamma.loc[row,'B'] = 0.0\n",
    "        y_gamma.loc[row,'M'] = 0.0\n",
    "        y_gamma.loc[row,'N'] = 0.0\n",
    "    elif y_greeks.loc[row,'greek_gamma'] == 'H':\n",
    "        y_gamma.loc[row,'G'] = 0.0\n",
    "        y_gamma.loc[row,'H'] = 1.0\n",
    "        y_gamma.loc[row,'E'] = 0.0\n",
    "        y_gamma.loc[row,'F'] = 0.0\n",
    "        y_gamma.loc[row,'A'] = 0.0\n",
    "        y_gamma.loc[row,'B'] = 0.0\n",
    "        y_gamma.loc[row,'M'] = 0.0\n",
    "        y_gamma.loc[row,'N'] = 0.0\n",
    "    elif y_greeks.loc[row,'greek_gamma'] == 'E':\n",
    "        y_gamma.loc[row,'G'] = 0.0\n",
    "        y_gamma.loc[row,'H'] = 0.0\n",
    "        y_gamma.loc[row,'E'] = 1.0\n",
    "        y_gamma.loc[row,'F'] = 0.0\n",
    "        y_gamma.loc[row,'A'] = 0.0\n",
    "        y_gamma.loc[row,'B'] = 0.0\n",
    "        y_gamma.loc[row,'M'] = 0.0\n",
    "        y_gamma.loc[row,'N'] = 0.0\n",
    "    elif y_greeks.loc[row,'greek_gamma'] == 'F':\n",
    "        y_gamma.loc[row,'G'] = 0.0\n",
    "        y_gamma.loc[row,'H'] = 0.0\n",
    "        y_gamma.loc[row,'E'] = 0.0\n",
    "        y_gamma.loc[row,'F'] = 1.0\n",
    "        y_gamma.loc[row,'A'] = 0.0\n",
    "        y_gamma.loc[row,'B'] = 0.0\n",
    "        y_gamma.loc[row,'M'] = 0.0\n",
    "        y_gamma.loc[row,'N'] = 0.0\n",
    "    elif y_greeks.loc[row,'greek_gamma'] == 'A':\n",
    "        y_gamma.loc[row,'G'] = 0.0\n",
    "        y_gamma.loc[row,'H'] = 0.0\n",
    "        y_gamma.loc[row,'E'] = 0.0\n",
    "        y_gamma.loc[row,'F'] = 0.0\n",
    "        y_gamma.loc[row,'A'] = 1.0\n",
    "        y_gamma.loc[row,'B'] = 0.0\n",
    "        y_gamma.loc[row,'M'] = 0.0\n",
    "        y_gamma.loc[row,'N'] = 0.0\n",
    "    elif y_greeks.loc[row,'greek_gamma'] == 'B':\n",
    "        y_gamma.loc[row,'G'] = 0.0\n",
    "        y_gamma.loc[row,'H'] = 0.0\n",
    "        y_gamma.loc[row,'E'] = 0.0\n",
    "        y_gamma.loc[row,'F'] = 0.0\n",
    "        y_gamma.loc[row,'A'] = 0.0\n",
    "        y_gamma.loc[row,'B'] = 1.0\n",
    "        y_gamma.loc[row,'M'] = 0.0\n",
    "        y_gamma.loc[row,'N'] = 0.0\n",
    "    elif y_greeks.loc[row,'greek_gamma'] == 'M':\n",
    "        y_gamma.loc[row,'G'] = 0.0\n",
    "        y_gamma.loc[row,'H'] = 0.0\n",
    "        y_gamma.loc[row,'E'] = 0.0\n",
    "        y_gamma.loc[row,'F'] = 0.0\n",
    "        y_gamma.loc[row,'A'] = 0.0\n",
    "        y_gamma.loc[row,'B'] = 0.0\n",
    "        y_gamma.loc[row,'M'] = 1.0\n",
    "        y_gamma.loc[row,'N'] = 0.0\n",
    "    else:\n",
    "        y_gamma.loc[row,'G'] = 0.0\n",
    "        y_gamma.loc[row,'H'] = 0.0\n",
    "        y_gamma.loc[row,'E'] = 0.0\n",
    "        y_gamma.loc[row,'F'] = 0.0\n",
    "        y_gamma.loc[row,'A'] = 0.0\n",
    "        y_gamma.loc[row,'B'] = 0.0\n",
    "        y_gamma.loc[row,'M'] = 0.0\n",
    "        y_gamma.loc[row,'N'] = 1.0\n",
    "print(y_gamma.head())\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_transformed, y_gamma, test_size=0.1, random_state=42, stratify=y)\n",
    "\n",
    "## Radical new approach, try tuning with all the data against just the \n",
    "## test with one run per day, and model improvements using early stopping\n",
    "##\n",
    "print (\"Compiling models\")\n",
    "\n",
    "## notes when run with seed 14242\n",
    "## MLP single model 1/1 [==============================] - 0s 40ms/step - loss: 21.1738 - accuracy: 0.9635 - binary_accuracy: 0.9635 - val_loss: 15.7028 - val_accuracy: 0.8952 - val_binary_accuracy: 0.8952\n",
    "## CNN single model 1/1 [==============================] - 0s 83ms/step - loss: 24.7791 - accuracy: 0.9736 - binary_accuracy: 0.9736 - val_loss: 15.8823 - val_accuracy: 0.8468 - val_binary_accuracy: 0.8468\n",
    "## LSTM single model 1/1 [==============================] - 0s 82ms/step - loss: 35.5633 - accuracy: 0.5000 - binary_accuracy: 0.6908 - val_loss: 16.8421 - val_accuracy: 0.5000 - val_binary_accuracy: 0.6613\n",
    "\n",
    "modelno = 0\n",
    "for modeltup in ggmodels:\n",
    "    #X_train, X_val, y_train, y_val = train_test_split(X_transformed, y, test_size=0.2, random_state=42+(modelno%2)*100, stratify=y)\n",
    "    model = modeltup[0]\n",
    "    print (\"Compiling and fitting model\", modelno+1)\n",
    "    tf.random.set_seed(91+modelno*79)\n",
    "    #(optimizer='adam', loss = balanced_log_loss)\n",
    "    if modeltup[1] == 2: # this is for LSTM\n",
    "        model.compile(loss=balanced_log_loss_lstm,\n",
    "                optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "                metrics=['accuracy', 'binary_accuracy'])\n",
    "        Xd = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1])) # only for LSTM\n",
    "        Xd_val = np.reshape(X_val, (X_val.shape[0], 1, X_val.shape[1])) # only for LSTM\n",
    "        history = model.fit(\n",
    "            X_transformed, #Xd, \n",
    "            y, #y_train,\n",
    "            epochs = 1500,\n",
    "            batch_size = len(Xd),\n",
    "            validation_data = (Xd_val, y_val),\n",
    "            callbacks=[early_stopping_lstm]\n",
    "        )\n",
    "    else: # this covers both CNN and MLP\n",
    "        model.compile(loss=balanced_log_loss,\n",
    "                optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "                metrics=['accuracy'])\n",
    "        history = model.fit(\n",
    "            X_train, #X_transformed, #X_train, # for MLP and CNN, else Xd\n",
    "            y_train, #y, #y_train,\n",
    "            epochs = maxepochs[modelno], #2500, #225, \n",
    "            verbose = False, #devrun,\n",
    "            batch_size = 617, # len(X_transformed),\n",
    "            validation_data = (X_val, y_val),\n",
    "            #callbacks=[early_stopping_g]\n",
    "        )\n",
    "    modelno += 1\n",
    "    if devrun:\n",
    "        pd.DataFrame(history.history).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f84c6f1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Select one of the models for submission\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5beaad97",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Using MIscores on 0.05 test data and up to 2500 iterations, before using SMOTE, with MLP at size 128x64, here is the loss estimate\n",
    "\n",
    "At MIscore 0.1 with Total features = 6\n",
    "Feaures to use = ['DU', 'FL', 'BC', 'GL', 'BQ', 'AF']\n",
    "\n",
    "Assessing balanced log loss of  1\n",
    "20/20 [==============================] - 0s 2ms/step\n",
    "intermediate balanced log loss  tf.Tensor(0.20610846885276007, shape=(), dtype=float64)\n",
    "Assessing balanced log loss of  1\n",
    "20/20 [==============================] - 0s 2ms/step\n",
    "intermediate balanced log loss  tf.Tensor(0.3068458051364643, shape=(), dtype=float64)\n",
    "Assessing balanced log loss of  1\n",
    "20/20 [==============================] - 0s 1ms/step\n",
    "intermediate balanced log loss  tf.Tensor(0.22404914032602652, shape=(), dtype=float64)\n",
    "Assessing balanced log loss of  1\n",
    "20/20 [==============================] - 0s 1ms/step\n",
    "intermediate balanced log loss  tf.Tensor(0.23913039212074222, shape=(), dtype=float64)\n",
    "Assessing balanced log loss of  1\n",
    "20/20 [==============================] - 0s 2ms/step\n",
    "intermediate balanced log loss  tf.Tensor(0.3335314783429701, shape=(), dtype=float64)\n",
    "\n",
    "At MIscore 0.05 Total features = 9\n",
    "Feaures to use = ['DU', 'FL', 'BC', 'GL', 'BQ', 'AF', 'FD ', 'DI', 'AB']\n",
    "\n",
    "Assessing balanced log loss of  1\n",
    "20/20 [==============================] - 0s 2ms/step\n",
    "intermediate balanced log loss  tf.Tensor(0.13666516888732855, shape=(), dtype=float64)\n",
    "Assessing balanced log loss of  1\n",
    "20/20 [==============================] - 0s 2ms/step\n",
    "intermediate balanced log loss  tf.Tensor(0.11567961657836084, shape=(), dtype=float64)\n",
    "Assessing balanced log loss of  1\n",
    "20/20 [==============================] - 0s 2ms/step\n",
    "intermediate balanced log loss  tf.Tensor(0.09968245750887358, shape=(), dtype=float64)\n",
    "Assessing balanced log loss of  1\n",
    "20/20 [==============================] - 0s 2ms/step\n",
    "intermediate balanced log loss  tf.Tensor(0.08188260623813551, shape=(), dtype=float64)\n",
    "Assessing balanced log loss of  1\n",
    "20/20 [==============================] - 0s 1ms/step\n",
    "intermediate balanced log loss  tf.Tensor(0.097783840724294, shape=(), dtype=float64)\n",
    "\n",
    "At MIscore=0.01 Total features = 33\n",
    "Feaures to use = ['DU', 'FL', 'BC', 'GL', 'BQ', 'AF', 'FD ', 'DI', 'AB', 'CC', 'EH', 'DA', 'FR', 'BN', 'AM', 'CR', 'GH', 'AY', 'EE', 'DF', 'FE', 'DE', 'CL', 'GF', 'FC', 'BP', 'CF', 'EB', 'EU', 'CD ', 'FI', 'GE', 'DH']\n",
    "\n",
    "Assessing balanced log loss of  1\n",
    "20/20 [==============================] - 0s 2ms/step\n",
    "intermediate balanced log loss  tf.Tensor(0.1682810280943917, shape=(), dtype=float64)\n",
    "Assessing balanced log loss of  1\n",
    "20/20 [==============================] - 0s 2ms/step\n",
    "intermediate balanced log loss  tf.Tensor(0.12137677262643709, shape=(), dtype=float64)\n",
    "Assessing balanced log loss of  1\n",
    "20/20 [==============================] - 0s 2ms/step\n",
    "intermediate balanced log loss  tf.Tensor(0.18444699871451065, shape=(), dtype=float64)\n",
    "Assessing balanced log loss of  1\n",
    "20/20 [==============================] - 0s 2ms/step\n",
    "intermediate balanced log loss  tf.Tensor(0.15090392797691776, shape=(), dtype=float64)\n",
    "Assessing balanced log loss of  1\n",
    "20/20 [==============================] - 0s 2ms/step\n",
    "intermediate balanced log loss  tf.Tensor(0.1910776021856574, shape=(), dtype=float64)\n",
    "\n",
    "At MIscore=0.005 Total features = 39\n",
    "Feaures to use = ['DU', 'FL', 'BC', 'GL', 'BQ', 'AF', 'FD ', 'DI', 'AB', 'CC', 'EH', 'DA', 'FR', 'BN', 'AM', 'CR', 'GH', 'AY', 'EE', 'DF', 'FE', 'DE', 'CL', 'GF', 'FC', 'BP', 'CF', 'EB', 'EU', 'CD ', 'FI', 'GE', 'DH', 'AX', 'AH', 'BR', 'CW ', 'GI', 'AR']\n",
    "\n",
    "Assessing balanced log loss of  1\n",
    "20/20 [==============================] - 0s 2ms/step\n",
    "intermediate balanced log loss  tf.Tensor(0.11494086086269004, shape=(), dtype=float64)\n",
    "Assessing balanced log loss of  1\n",
    "20/20 [==============================] - 0s 2ms/step\n",
    "intermediate balanced log loss  tf.Tensor(0.1072447926134375, shape=(), dtype=float64)\n",
    "Assessing balanced log loss of  1\n",
    "20/20 [==============================] - 0s 2ms/step\n",
    "intermediate balanced log loss  tf.Tensor(0.1755618442445653, shape=(), dtype=float64)\n",
    "Assessing balanced log loss of  1\n",
    "20/20 [==============================] - 0s 2ms/step\n",
    "intermediate balanced log loss  tf.Tensor(0.1471342863573456, shape=(), dtype=float64)\n",
    "Assessing balanced log loss of  1\n",
    "20/20 [==============================] - 0s 1ms/step\n",
    "intermediate balanced log loss  tf.Tensor(0.19444498367297247, shape=(), dtype=float64)\n",
    "\n",
    "At MIscore=0.001 Total features = 40\n",
    "Feaures to use = ['DU', 'FL', 'BC', 'GL', 'BQ', 'AF', 'FD ', 'DI', 'AB', 'CC', 'EH', 'DA', 'FR', 'BN', 'AM', 'CR', 'GH', 'AY', 'EE', 'DF', 'FE', 'DE', 'CL', 'GF', 'FC', 'BP', 'CF', 'EB', 'EU', 'CD ', 'FI', 'GE', 'DH', 'AX', 'AH', 'BR', 'CW ', 'GI', 'AR', 'CS']\n",
    "\n",
    "Assessing balanced log loss of  1\n",
    "20/20 [==============================] - 0s 2ms/step\n",
    "intermediate balanced log loss  tf.Tensor(0.11340922224124753, shape=(), dtype=float64)\n",
    "Assessing balanced log loss of  1\n",
    "20/20 [==============================] - 0s 1ms/step\n",
    "intermediate balanced log loss  tf.Tensor(0.11931587248815605, shape=(), dtype=float64)\n",
    "Assessing balanced log loss of  1\n",
    "20/20 [==============================] - 0s 2ms/step\n",
    "intermediate balanced log loss  tf.Tensor(0.1869421876873575, shape=(), dtype=float64)\n",
    "Assessing balanced log loss of  1\n",
    "20/20 [==============================] - 0s 2ms/step\n",
    "intermediate balanced log loss  tf.Tensor(0.1360949628917216, shape=(), dtype=float64)\n",
    "Assessing balanced log loss of  1\n",
    "20/20 [==============================] - 0s 1ms/step\n",
    "intermediate balanced log loss  tf.Tensor(0.19814547587576914, shape=(), dtype=float64)\n",
    "\n",
    "Below this, the MIscores required are tiny and if any are included then all should probably be included."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e420a76a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Added a voter \n",
    "Voter on 5 of 15 with the biggest value of the majority being taken on a unanimous vote, the second biggest on 4 or 3 of 5 as major or simple majority. \n",
    "An attempt was made to find a value to use when the vote is unanimous but in \n",
    "the end a binary search failed to produce better results than taking the highest\n",
    "score and applying it for whichever way the vote ends up going unanimously.\n",
    "It is not yet clear if the 4 or 3 of 5 case will occur, and if so what the best\n",
    "weighted adjustment might be for a log loss function.\n",
    "\n",
    "First try, it helped bring 0.22 to 0.18 on the public score. However, helping over an average may not be guaranteed and for now it appears that votes are always unanimous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00616d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T20:31:42.04224Z",
     "iopub.status.busy": "2023-06-25T20:31:42.041827Z",
     "iopub.status.idle": "2023-06-25T20:31:42.425807Z",
     "shell.execute_reply": "2023-06-25T20:31:42.424704Z",
     "shell.execute_reply.started": "2023-06-25T20:31:42.042201Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate the predictions for Greek's Gamma output only\n",
    "# using ggmodel[10]\n",
    "\n",
    "#modeltup = ggmodels[8]\n",
    "#odel = modeltup[0]\n",
    "#gpred_submission = model.predict(X_transformed) # for MLP and CNN\n",
    "#gpred_submission = ggpred_submission.astype(np.float64)\n",
    "#_gamma_np = np.array(y_gamma)\n",
    "#intermediate_loss = balanced_log_loss(y_gamma_np, ggpred_submission)\n",
    "#print (\"Greek Gamma prediction balanced log loss\", intermediate_loss)\n",
    "#print(ggpred_submission)\n",
    "#print(y_gamma_np)\n",
    "\n",
    "ggpred_submission = np.zeros(len(X_transformed))\n",
    "\n",
    "for bag in range(bag_num):\n",
    "    for clf in lgbmodels[bag]:\n",
    "        ggpred_submission += clf.predict_proba(X_transformed)[:,1] / n_fold / bag_num\n",
    "\n",
    "print (ggpred_submission)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d484ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T20:31:42.427486Z",
     "iopub.status.busy": "2023-06-25T20:31:42.427215Z",
     "iopub.status.idle": "2023-06-25T20:31:45.614409Z",
     "shell.execute_reply": "2023-06-25T20:31:45.61349Z",
     "shell.execute_reply.started": "2023-06-25T20:31:42.427465Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#all_df = otrain.copy()\n",
    "\n",
    "#y = pd.DataFrame(all_df['Class'],columns=['class_0'])\n",
    "#y['class_1'] = all_df['Class'].fillna(0)\n",
    "#y['class_0'] = y['class_1'] * -1 + 1\n",
    "\n",
    "#print(y)\n",
    "\n",
    "#X_transformed = all_df.copy()\n",
    "#X_transformed.drop('Class', axis=1, inplace=True)\n",
    "#X_transformed.drop('Id', axis=1, inplace=True)\n",
    "#X_transformed = X_transformed[usefeats]\n",
    "#X_transformed = X_transformed.astype(np.float64)\n",
    "\n",
    "# run voter on models\n",
    "modelno = 0\n",
    "intermediate_loss = 100.0\n",
    "y = y.astype(np.float64)\n",
    "y_pred_ensemble = []\n",
    "loss_sorted_models = []\n",
    "for modeltup in models:\n",
    "    model = modeltup[0]\n",
    "    print (\"Assessing balanced log loss of \", modelno+1)\n",
    "    if modeltup[1] == 2:\n",
    "        Xd = np.reshape(X_transformed, (X_transformed.shape[0], 1, X_transformed.shape[1])) # for LTSM\n",
    "        pred_submission = model.predict(Xd)\n",
    "        pred_submission = pred_submission.astype(np.float64)\n",
    "        pred_submission = np.reshape(pred_submission, (pred_submission.shape[0], pred_submission.shape[2])) # only for LSTM\n",
    "        intermediate_loss = balanced_log_loss_lstm(y, pred_submission)\n",
    "    else:\n",
    "        pred_submission = model.predict(X_transformed) # for MLP and CNN\n",
    "        pred_submission = pred_submission.astype(np.float64)\n",
    "        intermediate_loss = balanced_log_loss(y, pred_submission)\n",
    "    y_pred_ensemble.append(pred_submission)\n",
    "    print (\"intermediate balanced log loss \", intermediate_loss)\n",
    "    loss_sorted_models.append([intermediate_loss, model])\n",
    "    \n",
    "loss_sorted_models = sorted(loss_sorted_models, key=lambda x: x[0])\n",
    "remodeled = []\n",
    "for remodel in loss_sorted_models:\n",
    "    for domodel in models:\n",
    "        if remodel[1] == domodel[0]:\n",
    "            remodeled.append([remodel[1], domodel[1]])\n",
    "            \n",
    "print(\"loss_sorted_models=\", loss_sorted_models)\n",
    "print(\"remodeled=\", remodeled)\n",
    "models = remodeled\n",
    "\n",
    "def voted_predictions(X_data):\n",
    "    # we ignore the first 5 and last 5 models since the likely contain outliers\n",
    "    # initialize the possible predictions with the first model, and prep the rest\n",
    "    pred_submission = []  # we will update this model according to votes & confidence\n",
    "    pred_submission2 = []\n",
    "    pred_submission3 = []\n",
    "    pred_submission4 = []\n",
    "    pred_submission5 = []\n",
    "    basemodel = 5\n",
    "    if models[basemodel][1] == 2:\n",
    "        Xd = np.reshape(X_data, (X_data.shape[0], 1, X_data.shape[1])) # for LTSM\n",
    "        pred_submission = models[basemodel][0].predict(Xd)\n",
    "        pred_submission = pred_submission.astype(np.float64)\n",
    "        pred_submission = np.reshape(pred_submission, (pred_submission.shape[0], pred_submission.shape[2])) # only for LSTM\n",
    "    else:\n",
    "        pred_submission = models[basemodel][0].predict(X_data) # for MLP and CNN\n",
    "        pred_submission = pred_submission.astype(np.float64)\n",
    "    # next model\n",
    "    if models[basemodel+1][1] == 2:\n",
    "        Xd = np.reshape(X_data, (X_data.shape[0], 1, X_data.shape[1])) # for LTSM\n",
    "        pred_submission2 = models[basemodel+1][0].predict(Xd)\n",
    "        pred_submission2 = pred_submission.astype(np.float64)\n",
    "        pred_submission2 = np.reshape(pred_submission, (pred_submission.shape[0], pred_submission.shape[2])) # only for LSTM\n",
    "    else:\n",
    "        pred_submission2 = models[basemodel+1][0].predict(X_data) # for MLP and CNN\n",
    "        pred_submission2 = pred_submission.astype(np.float64)\n",
    "    # next model\n",
    "    if models[basemodel+2][1] == 2:\n",
    "        Xd = np.reshape(X_data, (X_data.shape[0], 1, X_data.shape[1])) # for LTSM\n",
    "        pred_submission3 = models[basemodel+2][0].predict(Xd)\n",
    "        pred_submission3 = pred_submission.astype(np.float64)\n",
    "        pred_submission3 = np.reshape(pred_submission, (pred_submission.shape[0], pred_submission.shape[2])) # only for LSTM\n",
    "    else:\n",
    "        pred_submission3 = models[basemodel+2][0].predict(X_data) # for MLP and CNN\n",
    "        pred_submission3 = pred_submission.astype(np.float64)\n",
    "    if models[basemodel+3][1] == 2:\n",
    "        Xd = np.reshape(X_data, (X_data.shape[0], 1, X_data.shape[1])) # for LTSM\n",
    "        pred_submission4 = models[basemodel+3][0].predict(Xd)\n",
    "        pred_submission4 = pred_submission.astype(np.float64)\n",
    "        pred_submission4 = np.reshape(pred_submission, (pred_submission.shape[0], pred_submission.shape[2])) # only for LSTM\n",
    "    else:\n",
    "        pred_submission4 = models[basemodel+3][0].predict(X_data) # for MLP and CNN\n",
    "        pred_submission4 = pred_submission.astype(np.float64)\n",
    "    if models[basemodel+4][1] == 2:\n",
    "        Xd = np.reshape(X_data, (X_data.shape[0], 1, X_data.shape[1])) # for LTSM\n",
    "        pred_submission5 = models[basemodel+4][0].predict(Xd)\n",
    "        pred_submission5 = pred_submission.astype(np.float64)\n",
    "        pred_submission5 = np.reshape(pred_submission, (pred_submission.shape[0], pred_submission.shape[2])) # only for LSTM\n",
    "    else:\n",
    "        pred_submission5 = models[basemodel+4][0].predict(X_data) # for MLP and CNN\n",
    "        pred_submission5 = pred_submission.astype(np.float64)\n",
    "    # now that we have all 5 predictions for each test case find out the\n",
    "    # class and if it is by 3 (simple), 4 (major), or 5 (unanimous) vote.\n",
    "    class0votes5 = 0\n",
    "    class0votes4 = 0\n",
    "    class0votes3 = 0\n",
    "    class1votes5 = 0\n",
    "    class1votes4 = 0\n",
    "    class1votes3 = 0        \n",
    "    for row in range(len(pred_submission)):\n",
    "        voteForClass0 = 0\n",
    "        voteForClass1 = 0\n",
    "        if pred_submission[row][0] >= 0.50:\n",
    "            voteForClass0 += 1\n",
    "        else:\n",
    "            voteForClass1 += 1\n",
    "        if pred_submission2[row][0] >= 0.50:\n",
    "            voteForClass0 += 1\n",
    "        else:\n",
    "            voteForClass1 += 1\n",
    "        if pred_submission3[row][0] >= 0.50:\n",
    "            voteForClass0 += 1\n",
    "        else:\n",
    "            voteForClass1 += 1\n",
    "        if pred_submission4[row][0] >= 0.50:\n",
    "            voteForClass0 += 1\n",
    "        else:\n",
    "            voteForClass1 += 1\n",
    "        if pred_submission5[row][0] >= 0.50:\n",
    "            voteForClass0 += 1\n",
    "        else:\n",
    "            voteForClass1 += 1\n",
    "        confidences = [pred_submission[row][0], pred_submission2[row][0], pred_submission3[row][0], pred_submission4[row][0], pred_submission5[row][0]]\n",
    "        confidences.sort()\n",
    "        if voteForClass0 == 5:\n",
    "            class0votes5 += 1\n",
    "            pred_submission[row][0] = confidences[4] #0.975 # confidences[4]\n",
    "            pred_submission[row][1] = 1.0 - confidences[4] #0.025 # 1.0 - confidences[4]\n",
    "        if voteForClass0 == 4:\n",
    "            class0votes4 += 1\n",
    "            pred_submission[row][0] = confidences[3]\n",
    "            pred_submission[row][1] = 1.0 - confidences[3]\n",
    "        if voteForClass0 == 3:\n",
    "            class0votes3 += 1\n",
    "            pred_submission[row][0] = confidences[3]\n",
    "            pred_submission[row][1] = 1.0 - confidences[3]\n",
    "        if voteForClass1 == 5:\n",
    "            class1votes5 += 1\n",
    "            pred_submission[row][0] = confidences[0] #0.025 #confidences[0]\n",
    "            pred_submission[row][1] = 1.0 - confidences[0] #0.975 #1.0 - confidences[0]\n",
    "        if voteForClass1 == 4:\n",
    "            class1votes4 += 1\n",
    "            pred_submission[row][0] = confidences[1]\n",
    "            pred_submission[row][1] = 1.0 - confidences[1]\n",
    "        if voteForClass1 == 3:\n",
    "            class1votes3 += 1\n",
    "            pred_submission[row][0] = confidences[1]\n",
    "            pred_submission[row][1] = 1.0 - confidences[1]\n",
    "    print(class0votes5,\"unanimous votes cast for class 0\")\n",
    "    print(class0votes4,\"majority votes cast for class 0\")\n",
    "    print(class0votes3,\"simple votes cast for class 0\")\n",
    "    print(class1votes5,\"unanimous votes cast for class 1\")\n",
    "    print(class1votes4,\"majority votes cast for class 1\")\n",
    "    print(class1votes3,\"simple votes cast for class 1\")\n",
    "\n",
    "    return pred_submission\n",
    "\n",
    "def accuracy_metric(pred_submission, y):\n",
    "    correctCount = 0\n",
    "    for row in range(len(pred_submission)):\n",
    "        if pred_submission[row][0] >= 0.5 and y[row][0] == 1:\n",
    "            correctCount += 1\n",
    "        if pred_submission[row][0] < 0.5 and y[row][0] == 0:\n",
    "            correctCount += 1\n",
    "    return correctCount/len(pred_submission)\n",
    "\n",
    "# print out the voted on predictions\n",
    "voted_pred = voted_predictions(X_transformed)\n",
    "voted_balancedlogloss = balanced_log_loss(y, voted_pred)\n",
    "print(\"+++++++++++++++++++++++ Voted average loss =\", voted_balancedlogloss)\n",
    "print(\"+++++++++++++++++++++++ Voted accuracy =\", accuracy_metric(pred_submission, y))\n",
    "\n",
    "# for now, will only work on MLP and CNN\n",
    "if models[0][1] != 2:\n",
    "    pred_submission = np.mean(y_pred_ensemble, axis=0)\n",
    "    internal_avg_loss = balanced_log_loss(y, pred_submission)\n",
    "    print(\"******************** Internal average loss =\", internal_avg_loss)\n",
    "    \n",
    "total0 = 0\n",
    "total1 = 0\n",
    "correct0 = 0\n",
    "correct1 = 0\n",
    "pred0 = 0\n",
    "pred1 = 0\n",
    "\n",
    "for row in range(len(voted_pred)):\n",
    "    if y[row][0] == 1:\n",
    "        total0 += 1\n",
    "    else:\n",
    "        total1 += 1\n",
    "    if y[row][0] >= 0.50:\n",
    "        if pred_submission[row][0] >= 0.50:\n",
    "            correct0 += 1\n",
    "    if y[row][0] < 0.50:\n",
    "        if pred_submission[row][0] < 0.50:\n",
    "            correct1 += 1\n",
    "    if pred_submission[row][0] >= 0.50:\n",
    "        pred0 += 1\n",
    "    if pred_submission[row][0] < 0.50:\n",
    "        pred1 += 1\n",
    "        \n",
    "print (\"Total true class0 =\", total0)\n",
    "print (\"Total true class1 =\", total1)\n",
    "print (\"Total correct class0 =\", correct0)\n",
    "print (\"Total correct class1 =\", correct1)\n",
    "print (\"Total predicted class0 =\", pred0)\n",
    "print (\"Total predicted class1 =\", pred1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9085d507",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T20:31:45.616684Z",
     "iopub.status.busy": "2023-06-25T20:31:45.616053Z",
     "iopub.status.idle": "2023-06-25T20:31:45.628656Z",
     "shell.execute_reply": "2023-06-25T20:31:45.627681Z",
     "shell.execute_reply.started": "2023-06-25T20:31:45.616655Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "total0 = 0\n",
    "total1 = 0\n",
    "correct0 = 0\n",
    "correct1 = 0\n",
    "pred0 = 0\n",
    "pred1 = 0\n",
    "\n",
    "for row in range(len(voted_pred)):\n",
    "    if y[row][0] == 1:\n",
    "        total0 += 1\n",
    "    else:\n",
    "        total1 += 1\n",
    "    if y[row][0] >= 0.50:\n",
    "        if ggpred_submission[row] < 0.50:\n",
    "            correct0 += 1\n",
    "    if y[row][0] < 0.50:\n",
    "        if ggpred_submission[row] >= 0.50:\n",
    "            correct1 += 1\n",
    "    if ggpred_submission[row] < 0.50:\n",
    "        pred0 += 1\n",
    "    if ggpred_submission[row] >= 0.50:\n",
    "        pred1 += 1\n",
    "        \n",
    "print (\"Total true class0 =\", total0)\n",
    "print (\"Total true class1 =\", total1)\n",
    "print (\"Total correct class0 =\", correct0)\n",
    "print (\"Total correct class1 =\", correct1)\n",
    "print (\"Total predicted class0 =\", pred0)\n",
    "print (\"Total predicted class1 =\", pred1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb632583",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T18:36:26.580923Z",
     "iopub.status.busy": "2023-06-25T18:36:26.580503Z",
     "iopub.status.idle": "2023-06-25T18:36:26.605795Z",
     "shell.execute_reply": "2023-06-25T18:36:26.604713Z",
     "shell.execute_reply.started": "2023-06-25T18:36:26.580887Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# code that analyzes the training test set to see if \n",
    "# using Greeks Gamma results might correct for false positives and negatives\n",
    "\n",
    "# we are definitely seeing cases where the percentage confidence of any one \n",
    "# category in a row does not reach 50%. Rethink this to pick the highest \n",
    "# of the categories and see if using this category's class improves \n",
    "# results or not. YOU ARE HERE\n",
    "\n",
    "def gghighscorer(ggrow):\n",
    "    highscore = -1\n",
    "    highscorer = 0\n",
    "    for elem in range(len(ggrow)):\n",
    "        if ggrow[elem]>highscore:\n",
    "            highscorer = elem\n",
    "            highscore = ggrow[elem]\n",
    "    return highscorer\n",
    "\n",
    "gghelper = [0, 0, 0, 0, 0, 0, 0, 0]\n",
    "total0 = 0\n",
    "total1 = 0\n",
    "\n",
    "for row in range(len(voted_pred)):\n",
    "    highscorer = gghighscorer(ggpred_submission[row])\n",
    "    if y[row][0] == 1:\n",
    "        total0 += 1\n",
    "    else:\n",
    "        total1 += 1\n",
    "    if y[row][0] >= 0.50:\n",
    "        if highscorer < 6:\n",
    "            gghelper[highscorer] -= 1\n",
    "        else:\n",
    "            gghelper[highscorer] += 1\n",
    "    else:\n",
    "        if highscorer < 6:\n",
    "            gghelper[highscorer] += 1\n",
    "        else:\n",
    "            gghelper[highscorer] -= 1\n",
    "\n",
    "print (\"gghelp order G,H,E,F,A,B,M,N =\", gghelper)\n",
    "print (\"Total class0 =\", total0)\n",
    "print (\"Total class1 =\", total1)\n",
    "print (y)\n",
    "\n",
    "gghelper = [0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "for row in range(len(voted_pred)):\n",
    "    highscorer = gghighscorer(ggpred_submission[row])\n",
    "    if voted_pred[row][0] >= 0.50: # can greeks gamma do better?\n",
    "        if y[row][0] < 0.50: \n",
    "            if highscorer < 6:\n",
    "                gghelper[highscorer] += 1\n",
    "        else:\n",
    "            if highscorer < 6:\n",
    "                gghelper[highscorer] -= 1\n",
    "    else: #voted_pred[row][0] < 0.50: # can greeks gamma do better?\n",
    "        if y[row][0] >= 0.50:\n",
    "            if highscorer >= 6:\n",
    "                gghelper[highscorer] += 1\n",
    "        else:\n",
    "            if highscorer >= 6:\n",
    "                gghelper[highscorer] -= 1\n",
    "\n",
    "print (\"gghelp order G,H,E,F,A,B,M,N =\", gghelper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d9cb25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T20:31:45.630316Z",
     "iopub.status.busy": "2023-06-25T20:31:45.630021Z",
     "iopub.status.idle": "2023-06-25T20:31:45.816361Z",
     "shell.execute_reply": "2023-06-25T20:31:45.815436Z",
     "shell.execute_reply.started": "2023-06-25T20:31:45.630291Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds = np.zeros(len(test))\n",
    "\n",
    "for bag in range(bag_num):\n",
    "    for clf in lgbmodels[bag]:\n",
    "            preds += clf.predict_proba(test[usefeats])[:,1] / n_fold / bag_num\n",
    "            \n",
    "my_submission = test[['Id']].copy()\n",
    "my_submission['Class_0'] = 1-preds\n",
    "my_submission['Class_1'] = preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595911a4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a80123",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T20:18:38.488021Z",
     "iopub.status.busy": "2023-06-25T20:18:38.487484Z",
     "iopub.status.idle": "2023-06-25T20:18:39.030794Z",
     "shell.execute_reply": "2023-06-25T20:18:39.029741Z",
     "shell.execute_reply.started": "2023-06-25T20:18:38.487983Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "import math\n",
    "\n",
    "print (test.head())\n",
    "\n",
    "test_data_normal = test.copy()\n",
    "test_data_normal.drop('Id', axis=1, inplace=True)\n",
    "test_data_normal = test_data_normal[usefeats]\n",
    "test_data_normal = np.array(test_data_normal)\n",
    "\n",
    "if models[0][1] == 2:\n",
    "    Xd_test = np.reshape(test_data_normal, (test_data_normal.shape[0], 1, test_data_normal.shape[1])) # only for LSTM\n",
    "    y_pred = voted_predictions(Xd_test)\n",
    "    #y_pred = models[2][0].predict(Xd_test)\n",
    "else:\n",
    "    y_pred = voted_predictions(test_data_normal)\n",
    "    #y_pred = models[2][0].predict(test_data_normal)\n",
    "\n",
    "my_submission = test.copy()\n",
    "my_submission = my_submission[['Id']]\n",
    "\n",
    "my_submission[['class_0']] = 0.0\n",
    "my_submission[['class_1']] = 0.0\n",
    "for row in range(len(y_pred)):\n",
    "    if models[0][1] == 2:\n",
    "        my_submission.loc[row,'class_0'] = y_pred[row][0][0]\n",
    "        my_submission.loc[row,'class_1'] = y_pred[row][0][1]\n",
    "\n",
    "    else:\n",
    "        my_submission.loc[row,'class_0'] = y_pred[row][0]\n",
    "        my_submission.loc[row,'class_1'] = y_pred[row][1]\n",
    "\n",
    "for row in range(len(my_submission)): # sanity check so none are out of range\n",
    "    if math.isnan(my_submission.loc[row,'class_0']):\n",
    "        my_submission.loc[row,'class_1'] = 1.0-(1e-15)\n",
    "        my_submission.loc[row,'class_0'] = 1e-15\n",
    "        print (\"Row\", row, \"isnan in class_0\")\n",
    "    if math.isnan(my_submission.loc[row,'class_1']):\n",
    "        my_submission.loc[row,'class_1'] = 1.0-(1e-15)\n",
    "        my_submission.loc[row,'class_0'] = 1e-15\n",
    "        print (\"Row\", row, \"isnan in class_1\")\n",
    "    if my_submission.loc[row,'class_0'] > 1.0-(1e-15):\n",
    "        my_submission.loc[row,'class_0'] = 1.0-(1e-15)\n",
    "        my_submission.loc[row,'class_1'] = 1e-15\n",
    "        print (\"Row\", row, \"is over 1 in class_0\")\n",
    "    if my_submission.loc[row,'class_1'] > 1.0-(1e-15):\n",
    "        my_submission.loc[row,'class_1'] = 1.0-(1e-15)\n",
    "        my_submission.loc[row,'class_0'] = 1e-15\n",
    "        print (\"Row\", row, \"is over 1 in class_1\")\n",
    "    if my_submission.loc[row,'class_0'] < 1e-15:\n",
    "        my_submission.loc[row,'class_0'] = 1e-15\n",
    "        my_submission.loc[row,'class_1'] = 1.0-(1e-15)\n",
    "        print (\"Row\", row, \"is too close to 0 in class_0\")\n",
    "    if my_submission.loc[row,'class_1'] < 1e-15:\n",
    "        my_submission.loc[row,'class_1'] = 1e-15\n",
    "        my_submission.loc[row,'class_0'] = 1.0-(1e-15)\n",
    "        print (\"Row\", row, \"is too close to 0 in class_1\")\n",
    "    # The attempt below to correct Class_1 to be Class_0 for Gamma cases\n",
    "    # where  Gamma is M or N (last two categories) failed - loss \n",
    "    # went from .2 to .99\n",
    "    #if my_submission.loc[row,'class_1'] > 0.50:\n",
    "    #    # See if Greek's Gamma suggests swappinng to class_0\n",
    "    #    highscorer = gghighscorer(ggpred_submission[row])\n",
    "    #    if highscorer >= 6: # only consider M and N which map to class 0\n",
    "    #        if gghelper[highscorer] > 0: # training concluded it might help\n",
    "    #            holdClass0 = my_submission.loc[row,'class_0']\n",
    "    #            my_submission.loc[row,'class_0'] = my_submission.loc[row,'class_1']\n",
    "    #           my_submission.loc[row,'class_1'] = holdClass0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169d5520",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T20:31:45.818028Z",
     "iopub.status.busy": "2023-06-25T20:31:45.81774Z",
     "iopub.status.idle": "2023-06-25T20:31:45.824996Z",
     "shell.execute_reply": "2023-06-25T20:31:45.824024Z",
     "shell.execute_reply.started": "2023-06-25T20:31:45.818004Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if devrun:\n",
    "    print(my_submission.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614a134d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T20:31:45.826753Z",
     "iopub.status.busy": "2023-06-25T20:31:45.826482Z",
     "iopub.status.idle": "2023-06-25T20:31:45.840397Z",
     "shell.execute_reply": "2023-06-25T20:31:45.83938Z",
     "shell.execute_reply.started": "2023-06-25T20:31:45.82673Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_submission.to_csv('submission.csv', index=False)\n",
    "print (\"Submission saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 14.467638,
   "end_time": "2023-08-11T01:39:27.129872",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-08-11T01:39:12.662234",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
